# step2: tokenize (Megatron-LM preprocess_data.py or MindSpeed-compatible)

# IMPORTANT: input/output should live under DATAPOOL_ROOT (single source of truth).
INPUT_GLOB="${DATAPOOL_ROOT}/data/raw/*.jsonl"
TOKENIZER_MODEL="/home/unlimitediw/workspace/models/Qwen3-1.7B/"
# Archive outputs into datapool/data
OUTPUT_PREFIX="${DATAPOOL_ROOT}/data/tokenized/${RUN_ID}/megatron/qw"

# Repo dir containing tools/preprocess_data.py
MEGATRON_DIR="/home/unlimitediw/workspace/Megatron-LM"

# Tune for CPU
WORKERS=16
PARTITIONS=16
LOG_INTERVAL=100000

JSON_KEYS="text"
TOKENIZER_TYPE="HuggingFaceTokenizer"

